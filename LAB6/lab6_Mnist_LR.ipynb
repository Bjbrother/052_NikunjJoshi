{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6_Mnist_LR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIZr9TGaytxr"
      },
      "source": [
        "#Importing libraries\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import io\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p29gDhDf1_5K",
        "outputId": "3ed029c0-4d96-4a64-bad6-4853c98a2edf"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Convert to float32.\n",
        "x_train = np.array(x_train, np.float32)\n",
        "x_test = np.array(x_test, np.float32)\n",
        "\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28).\n",
        "num_features=784\n",
        "x_train = x_train.reshape(60000, num_features)\n",
        "x_test = x_test.reshape(10000, num_features)\n",
        "\n",
        "\n",
        "# Normalize images value from [0, 255] to [0, 1].\n",
        "x_train = x_train / 255\n",
        "x_test = x_test /255\n",
        "\n",
        "x_train"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEaKEaoG2DPn",
        "outputId": "20659b29-3f65-46cf-f4a4-4fb614e8e805"
      },
      "source": [
        "x_test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZJq_0Gb2FNT",
        "outputId": "49d1fdc8-4841-43ca-921d-eb85ae74eca8"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLm1ftNc2L0h",
        "outputId": "1bfcd1f6-0bd8-4748-9f1e-76e41879297e"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCA4AHHP2PJ-",
        "outputId": "5c4b53e5-4be1-4c3d-d311-1f3138a6bc85"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxjDVALz2SqX"
      },
      "source": [
        "#setting up hyperparameters and data set parameters\n",
        "\n",
        "#initialize model parameters\n",
        "#num_class = number of outputs (10) (0 to 9 digits)\n",
        "#num_features = number of input para (784)\n",
        "\n",
        "# MNIST dataset parameters.\n",
        "\n",
        "num_classes = 10 # 0 to 9 digits\n",
        "\n",
        "num_features = 784 # 28*28\n",
        "\n",
        "# Training parameters.\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "display_step = 50"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06vVdgJ32Vy1"
      },
      "source": [
        "# Use tf.data API to shuffle and batch data.\n",
        "num_batches = int(x_train.shape[0] / batch_size)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxFW7Srs2bKY"
      },
      "source": [
        "#initializing weights and biases\n",
        "#with ones and zeros\n",
        "\n",
        "# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n",
        "\n",
        "W = tf.Variable(np.random.randn(784, 10).astype(np.float32))\n",
        "\n",
        "# Bias of shape [10], the total number of classes.\n",
        "B = tf.Variable(np.random.randn(10).astype(np.float32))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7nXrvpf2gZJ"
      },
      "source": [
        "#defining logistic regression and cost function\n",
        "\n",
        "#which converts the inputs into a probability distribution proportional to the exponents of the inputs using the softmax function. \n",
        "#The softmax function, which is implemented using the function tf.nn.softmax, also makes sure that the sum of all the inputs equals one.\n",
        "\n",
        "# Logistic regression (Wx + b).\n",
        "\n",
        "def logistic_regression(x):\n",
        "\n",
        "    # Apply softmax to normalize the logits to a probability distribution.\n",
        "    return tf.nn.softmax(tf.add(tf.matmul(x, W), B))\n",
        "    \n",
        "\n",
        "# Cross-Entropy loss function.\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "\n",
        "    # Encode label to a one hot vector.\n",
        "    y_true = tf.one_hot(y_true, depth = num_classes)\n",
        "    \n",
        "\n",
        "    # Clip prediction values to avoid log(0) error.\n",
        "\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)    \n",
        "\n",
        "    # Compute cross-entropy.\n",
        "    loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
        "    return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjaAX3Xu2j4x"
      },
      "source": [
        "# Accuracy metric.\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "\n",
        "  # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
        "\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVxK14hC2nZp"
      },
      "source": [
        "Optimization Process and Updating Weights and Biases\n",
        "\n",
        "Now we define run_optimization() method where we update the weights of our model. We calculate the predictions using the logistic_regression(x) method by taking inputs and find out the loss generated by comparing the predicted value and the original value present in the data set. Next, we compute the gradients using and update the weights of the model with our stochastic gradient descent optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2gFKdER2tJA"
      },
      "source": [
        "# Optimization process. \n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "def run_optimization(x, y):\n",
        "\n",
        "# Wrap computation inside a GradientTape for automatic differentiation.\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = logistic_regression(x)\n",
        "        loss = cross_entropy(pred, y)\n",
        "\n",
        "    # Compute gradients.\n",
        "\n",
        "    gradients = g.gradient(loss, [W, B])\n",
        "    # Stochastic gradient descent optimizer.\n",
        "    # Update W and b following gradients.\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, [W, B]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhP-OWBd2xMu",
        "outputId": "8d0a8c9a-8dbd-44a0-dbae-2853cfa5be7d"
      },
      "source": [
        "# Run training for the given number of steps.\n",
        "\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "\n",
        "    # Run the optimization to update W and b values.\n",
        "\n",
        "    run_optimization(batch_x, batch_y)\n",
        "    if step % display_step == 0:\n",
        "\n",
        "        #Obtain Predictions\n",
        "        pred = logistic_regression(batch_x)\n",
        "        #Ccompute loss\n",
        "        loss = cross_entropy(pred, batch_y)\n",
        "        #Compute Accuracy\n",
        "        acc = accuracy(pred, batch_y)\n",
        "        #print accuracy\n",
        "        print(f\"step: {step}, loss: {loss}, accuracy: {acc}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 50, loss: 220.14715576171875, accuracy: 0.8203125\n",
            "step: 100, loss: 178.2867431640625, accuracy: 0.84765625\n",
            "step: 150, loss: 143.408203125, accuracy: 0.85546875\n",
            "step: 200, loss: 139.97572326660156, accuracy: 0.87109375\n",
            "step: 250, loss: 106.55428314208984, accuracy: 0.9140625\n",
            "step: 300, loss: 132.14854431152344, accuracy: 0.85546875\n",
            "step: 350, loss: 226.07354736328125, accuracy: 0.83984375\n",
            "step: 400, loss: 62.558685302734375, accuracy: 0.93359375\n",
            "step: 450, loss: 128.35804748535156, accuracy: 0.875\n",
            "step: 500, loss: 80.87374877929688, accuracy: 0.91796875\n",
            "step: 550, loss: 110.55818176269531, accuracy: 0.91015625\n",
            "step: 600, loss: 142.197509765625, accuracy: 0.87890625\n",
            "step: 650, loss: 64.973388671875, accuracy: 0.9375\n",
            "step: 700, loss: 70.18213653564453, accuracy: 0.93359375\n",
            "step: 750, loss: 105.34352111816406, accuracy: 0.890625\n",
            "step: 800, loss: 105.02420806884766, accuracy: 0.90625\n",
            "step: 850, loss: 121.63016510009766, accuracy: 0.89453125\n",
            "step: 900, loss: 98.333251953125, accuracy: 0.91796875\n",
            "step: 950, loss: 94.31513977050781, accuracy: 0.93359375\n",
            "step: 1000, loss: 97.02373504638672, accuracy: 0.921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTsPfFut231h",
        "outputId": "d5731a4e-631a-408b-a974-d69ecc081d59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test model on validation set.\n",
        "pred = logistic_regression(x_test)\n",
        "a = accuracy(pred, y_test)\n",
        "print(f\"Test Accuracy: {a}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9034000039100647\n"
          ]
        }
      ]
    }
  ]
}